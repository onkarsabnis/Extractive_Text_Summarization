{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive_Text_Summarization_Part2\n",
    "As the name suggests, text summarization summarizes long text into bite sized pieces based on the approach we choose.\n",
    "\n",
    "In addition to the benefit of summarizing large texts, this technique can even reduce the human bias when summarizing text. Think about it; think about the last time you were asked to summarize a book or a story – there’s always a degree of subjectivity. \n",
    "\n",
    "We could use text summarization to condense things like news articles, survey data, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches to text summarization. You can either go **extract route** or the **abstract route**.  With the extract approach, we use text features, e.g. syntax, word frequencies, word weights, etc to summarize text. With the abstract approach, we use advanced modeling techniques e.g neural networks to summarize text. In this notebook, we will focus on the **extractive** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "####To generate Colored Text\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "#print(color.BOLD + 'Hello World !' + color.END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\LENOVO\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Collecting Cython==0.29.14\n",
      "  Using cached Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.17.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-3.0.0.tar.gz (113 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107102 sha256=99ee3a6ba94975a932ae563dc34dbce384cfa3d2494251cff2aa544ec11a8694\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\83\\a6\\12\\bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
      "Successfully built smart-open\n",
      "Installing collected packages: Cython, smart-open, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.21\n",
      "    Uninstalling Cython-0.29.21:\n",
      "      Successfully uninstalled Cython-0.29.21\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-3.0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.summarization.summarizer import summarize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from heapq import nlargest\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sample we will use in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''This introduction aims to tell the story of how we\n",
    "put words into computers. It is part of the story of the field of\n",
    "natural language processing (NLP), a branch of artificial\n",
    "intelligence. It targets a wide audience with a basic\n",
    "understanding of computer programming, but avoids a detailed\n",
    "mathematical treatment, and it does not present any algorithms.\n",
    "It also does not focus on any particular application of NLP such\n",
    "as translation, question answering, or information extraction.\n",
    "The ideas presented here were developed by many researchers over\n",
    "many decades, so the citations are not exhaustive but rather direct\n",
    "the reader to a handful of papers that are, in the author's view,\n",
    "seminal. After reading this document, you should have a general\n",
    "understanding of word vectors (also known as word embeddings): why\n",
    "they exist, what problems they solve, where they come from, how\n",
    "they have changed over time, and what some of the open questions\n",
    "about them are. Readers already familiar with word vectors are\n",
    "advised to skip to Section 5 for the discussion of the most recent\n",
    "advance, contextual word vectors'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: GloVe Word Embeddings (TextRank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are vector representations of a given word. The reason we want to use word embeddings for text summarization is so we can tell the similarities and differences amongst the words in the sentences in the paragraphs and assign weights accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This introduction aims to tell the story of how we\\nput words into computers.',\n",
       " 'It is part of the story of the field of\\nnatural language processing (NLP), a branch of artificial\\nintelligence.',\n",
       " 'It targets a wide audience with a basic\\nunderstanding of computer programming, but avoids a detailed\\nmathematical treatment, and it does not present any algorithms.',\n",
       " 'It also does not focus on any particular application of NLP such\\nas translation, question answering, or information extraction.',\n",
       " \"The ideas presented here were developed by many researchers over\\nmany decades, so the citations are not exhaustive but rather direct\\nthe reader to a handful of papers that are, in the author's view,\\nseminal.\",\n",
       " 'After reading this document, you should have a general\\nunderstanding of word vectors (also known as word embeddings): why\\nthey exist, what problems they solve, where they come from, how\\nthey have changed over time, and what some of the open questions\\nabout them are.',\n",
       " 'Readers already familiar with word vectors are\\nadvised to skip to Section 5 for the discussion of the most recent\\nadvance, contextual word vectors']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split paragraph into sentences. We want to know how similar each sentence is with each other.\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction aims tell story we+put words computers.',\n",
       " 'part story field of+natural language processing (nlp), branch artificial+intelligence.',\n",
       " 'targets wide audience basic+understanding computer programming, avoids detailed+mathematical treatment, present algorithms.',\n",
       " 'also focus particular application nlp such+as translation, question answering, information extraction.',\n",
       " \"ideas presented developed many researchers over+many decades, citations exhaustive rather direct+the reader handful papers are, author's view,+seminal.\",\n",
       " 'reading document, general+understanding word vectors (also known word embeddings): why+they exist, problems solve, come from, how+they changed time, open questions+about are.',\n",
       " 'readers already familiar word vectors are+advised skip section 5 discussion recent+advance, contextual word vectors']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load stopwords\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    " \n",
    "#Pre-process your text. Remove punctuation, special characterts, numbers, etc. As the only thing we care\n",
    "#about are the actual words in the text.\n",
    "clean_sentences = [s.translate(string.punctuation) for s in sentences]\n",
    "clean_sentences  = [s.translate(string.digits) for s in clean_sentences]\n",
    "#lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    " \n",
    "#remove stopwords as for text summarization purposes,\n",
    "#these words add no value to word ranking.\n",
    "def remove_stopwords(sentence):\n",
    "    filtered_sentence = \" \".join([i for i in sentence if i not in stop])\n",
    "    return filtered_sentence\n",
    "clean_sentences = [remove_stopwords(s.split()) for s in clean_sentences]\n",
    "clean_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load in the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "file_ = open('glove.6B.300d.txt' , encoding = 'utf-8')\n",
    "for line in file_:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float')\n",
    "    word_embeddings[word] = coefs\n",
    "file_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #let’s compute the vector values on the words in our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        vector = sum([word_embeddings.get(w, np.zeros((300,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        vector = np.zeros((300,))\n",
    "    sentence_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compute the similarities between sentences, generate the scores based on the similarities and word embeddings and show the text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute sentence similaritiy, initiate with zeros\n",
    "similarity_matrix = np.zeros([len(sentences), len(sentences)])\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(similarity_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_graph = nx.from_numpy_matrix(similarity_matrix)\n",
    "scores = nx.pagerank(sim_graph)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary is:** \n",
      "After reading this document, you should have a general\n",
      "understanding of word vectors (also known as word embeddings): why\n",
      "they exist, what problems they solve, where they come from, how\n",
      "they have changed over time, and what some of the open questions\n",
      "about them are.\n",
      "It also does not focus on any particular application of NLP such\n",
      "as translation, question answering, or information extraction.\n",
      "The ideas presented here were developed by many researchers over\n",
      "many decades, so the citations are not exhaustive but rather direct\n",
      "the reader to a handful of papers that are, in the author's view,\n",
      "seminal.\n"
     ]
    }
   ],
   "source": [
    "#Sentence Ranking\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "#Choose desired number of sentences\n",
    "print(\"**Summary is:** \")\n",
    "for i in range(3):\n",
    "    print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_wordembed_textrank = []\n",
    "for i in range(3):\n",
    "    summary_wordembed_textrank.append(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Weighted Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we will compute weighted frequencies, similar to a weighted average. The weighted factor will based on the maximum frequency term in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split paragraph into sentences. We want to know how similar each sentence is with each other.\n",
    "sentences = sent_tokenize(text)\n",
    " \n",
    "#load stopwords\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    " \n",
    "clean_sentences = [s.translate( string.punctuation) for s in sentences] \n",
    "clean_sentences = [s.translate(string.digits) for s in clean_sentences] #lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "clean_sentences = [remove_stopwords(s.split()) for s in clean_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction aims tell story we+put words computers.',\n",
       " 'part story field of+natural language processing (nlp), branch artificial+intelligence.',\n",
       " 'targets wide audience basic+understanding computer programming, avoids detailed+mathematical treatment, present algorithms.',\n",
       " 'also focus particular application nlp such+as translation, question answering, information extraction.',\n",
       " \"ideas presented developed many researchers over+many decades, citations exhaustive rather direct+the reader handful papers are, author's view,+seminal.\",\n",
       " 'reading document, general+understanding word vectors (also known word embeddings): why+they exist, problems solve, come from, how+they changed time, open questions+about are.',\n",
       " 'readers already familiar word vectors are+advised skip section 5 discussion recent+advance, contextual word vectors']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the frequency of each word in our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "for i in range(len(clean_sentences)):\n",
    "    for word in nltk.word_tokenize(clean_sentences[i]):\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s get the maximum frequency and multiply this to each word frequency to get the weighted frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequency = max(word_frequencies.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the computed frequencies to our UNCLEANED sentences and then show our summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply scores to each UNCLEANED SENTENCE\n",
    "\n",
    "sentence_scores = {}\n",
    "for sent in sentences:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This introduction aims to tell the story of how we\\nput words into computers.': 0.06632653061224489,\n",
       " 'It is part of the story of the field of\\nnatural language processing (NLP), a branch of artificial\\nintelligence.': 0.1683673469387755,\n",
       " 'It targets a wide audience with a basic\\nunderstanding of computer programming, but avoids a detailed\\nmathematical treatment, and it does not present any algorithms.': 0.21938775510204084,\n",
       " 'It also does not focus on any particular application of NLP such\\nas translation, question answering, or information extraction.': 0.23469387755102042,\n",
       " 'Readers already familiar with word vectors are\\nadvised to skip to Section 5 for the discussion of the most recent\\nadvance, contextual word vectors': 0.19387755102040816}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It also does not focus on any particular application of NLP such\n",
      "as translation, question answering, or information extraction. It targets a wide audience with a basic\n",
      "understanding of computer programming, but avoids a detailed\n",
      "mathematical treatment, and it does not present any algorithms. Readers already familiar with word vectors are\n",
      "advised to skip to Section 5 for the discussion of the most recent\n",
      "advance, contextual word vectors It is part of the story of the field of\n",
      "natural language processing (NLP), a branch of artificial\n",
      "intelligence.\n"
     ]
    }
   ],
   "source": [
    "#Choose number of sentences you want in your summary\n",
    "summary_sentences = nlargest(4, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)\n",
    "summary_wtd_freq = summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Directly using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing (NLP), a branch of artificial\\nmathematical treatment, and it does not present any algorithms.\\nIt also does not focus on any particular application of NLP such\\nas translation, question answering, or information extraction.\\nunderstanding of word vectors (also known as word embeddings): why\\nthey have changed over time, and what some of the open questions\\nReaders already familiar with word vectors are\\nadvance, contextual word vectors'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_direct_using_gensim = summarize(text, ratio = 0.4)\n",
    "summary_direct_using_gensim\n",
    "#ratio (float, optional) – Number between 0 and 1 that determines the proportion of the number of sentences of the original text to be chosen for the summary.\n",
    "#word_count (int or None, optional) – Determines how many words will the output contain. If both parameters are provided, the ratio will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge score\n",
    "\n",
    "A good question after carriying this algorithm out, is deciding how to evaluate it. Was it a good summary? We can use something called a ROUGE score. ROUGE scores compare the contents of the summary to the contents of the original text. This will work the same way that computing recall and precision for non-text data sets work. In the context of ROUGE, we will be comparing n-grams betweent the summary and the original text. Recall will be computed as the division of the number of common ngrams over the total number of ngrams in the original text. Precision will be computed as the division of the number of common ngrams over the number of ngrams in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s start with unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Takes a string and removes stopwords.\n",
    "    \"\"\"\n",
    "    filtered_sentence = \" \".join([i for i in sentence if i not in stop_words])\n",
    "    return filtered_sentence\n",
    "\n",
    " \n",
    "def sanitize_text(sentence):\n",
    "    \"\"\"\n",
    "    Takes in a string and cleans it up.\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    #Replace all none alphanumeric characters with spaces\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "def generate_ngrams(sentence, n):\n",
    "    \"\"\"\n",
    "    Takes in a string and the number of ngrams you want to produce.\n",
    "    \"\"\"\n",
    "    #Clean text\n",
    "    sentence = sanitize_text(sentence)\n",
    "    #Split sentence into tokens\n",
    "    tokens = [token for token in word_tokenize(sentence) if token != \"\"]\n",
    "    #Create ngrams\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram_Rouge_score_of summary generated directly by gensim\n",
      "[0.43089430894308944, 1.0]\n",
      "F1 is :\n",
      "0.30113636363636365\n"
     ]
    }
   ],
   "source": [
    "#summary = \" \".join(summary)\n",
    "\n",
    "unigrams_sum = generate_ngrams(summary_direct_using_gensim, n=1)\n",
    "unigrams_orig = generate_ngrams(text, n= 1)\n",
    "unigrams_sum = set(unigrams_sum)\n",
    "unigrams_orig = set(unigrams_orig)\n",
    " \n",
    "matches = unigrams_sum.intersection(unigrams_orig)\n",
    "\n",
    "#Recall\n",
    "recall = float(len(matches)/len(unigrams_orig))\n",
    "#Precision\n",
    "precision = float(len(matches)/len(unigrams_sum))\n",
    "\n",
    "print(\"Unigram_Rouge_score_of summary generated directly by gensim\")\n",
    "print([recall,precision])\n",
    "print(\"F1 is :\")\n",
    "print(precision*recall/(precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The ideas presented here were developed by many researchers over\\nmany decades, so the citations are not exhaustive but rather direct\\nthe reader to a handful of papers that are, in the author's view,\\nseminal.\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Let’s look at bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBigram_Rouge_score_of summary generated directly by gensim\u001b[0m\n",
      "[0.3546511627906977, 0.9384615384615385]\n",
      "F1 is :\n",
      "0.25738396624472576\n"
     ]
    }
   ],
   "source": [
    "bigrams_sum = generate_ngrams(summary_direct_using_gensim, n=2)\n",
    "bigrams_orig = generate_ngrams(text, n= 2)\n",
    "bigrams_sum = set(bigrams_sum)\n",
    "bigrams_orig = set(bigrams_orig)\n",
    " \n",
    "matches = bigrams_sum.intersection(bigrams_orig)\n",
    "#Recall\n",
    "recall = float(len(matches)/len(bigrams_orig))\n",
    "#Precision\n",
    "precision = float(len(matches)/len(bigrams_sum))\n",
    "print(color.BOLD + \"Bigram_Rouge_score_of summary generated directly by gensim\" + color.END)\n",
    "print([recall,precision])\n",
    "print(\"F1 is :\")\n",
    "print(precision*recall/(precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_wordembed_textrank = \"\".join(summary_wordembed_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBigram_Rouge_score_of summary generated Wordembed Textrank\u001b[0m\n",
      "[0.5697674418604651, 0.98989898989899]\n",
      "F1 is :\n",
      "0.36162361623616235\n"
     ]
    }
   ],
   "source": [
    "bigrams_sum = generate_ngrams(summary_wordembed_textrank, n=2)\n",
    "bigrams_orig = generate_ngrams(text, n= 2)\n",
    "bigrams_sum = set(bigrams_sum)\n",
    "bigrams_orig = set(bigrams_orig)\n",
    " \n",
    "matches = bigrams_sum.intersection(bigrams_orig)\n",
    "#Recall\n",
    "recall = float(len(matches)/len(bigrams_orig))\n",
    "#Precision\n",
    "precision = float(len(matches)/len(bigrams_sum))\n",
    "print(color.BOLD + \"Bigram_Rouge_score_of summary generated Wordembed Textrank\" + color.END)\n",
    "print([recall,precision])\n",
    "print(\"F1 is :\")\n",
    "print(precision*recall/(precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBigram_Rouge_score_of summary generated by Weighted Freq\u001b[0m\n",
      "[0.45930232558139533, 0.9634146341463414]\n",
      "F1 is :\n",
      "0.3110236220472441\n"
     ]
    }
   ],
   "source": [
    "bigrams_sum = generate_ngrams(summary_wtd_freq, n=2)\n",
    "bigrams_orig = generate_ngrams(text, n= 2)\n",
    "bigrams_sum = set(bigrams_sum)\n",
    "bigrams_orig = set(bigrams_orig)\n",
    " \n",
    "matches = bigrams_sum.intersection(bigrams_orig)\n",
    "#Recall\n",
    "recall = float(len(matches)/len(bigrams_orig))\n",
    "#Precision\n",
    "precision = float(len(matches)/len(bigrams_sum))\n",
    "print(color.BOLD + \"Bigram_Rouge_score_of summary generated by Weighted Freq\" + color.END)\n",
    "print([recall,precision])\n",
    "print(\"F1 is :\")\n",
    "print(precision*recall/(precision + recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It would be better to pick the bigram score over the unigram score, mainly because bigrams carry slightly more context; hence, we can measure how much context from the original text is in the summary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyRouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
